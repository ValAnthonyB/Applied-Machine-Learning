{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c82b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fac92b",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c11e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df = pd.read_csv('C:/Users/vabalagon/Desktop/Machine Learning Projects/Applied-Machine-Learning-Projects/Customer Churn Prediction/data/processed.csv')\n",
    "\n",
    "# Get the features and target variable from the dataframe\n",
    "X = df.drop(['state', 'area_code', 'churn'], axis=1).to_numpy()\n",
    "y = df['churn'].to_numpy()\n",
    "\n",
    "# Split into training and testing parts\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.copy(), y, \n",
    "                                                    test_size = 0.25, \n",
    "                                                    shuffle=True, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y) #, stratify=y_smote\n",
    "# Apply SMOTE oversampling technique to the training set\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\n",
    "print('Resampled dataset shape %s' % Counter(y_train_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae61b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_per_class(classifier, X_test, y_test):\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    for y_i in np.unique(y_test)[::-1]:\n",
    "        print()\n",
    "\n",
    "        # Find the indices of y_i in the true labels\n",
    "        indices_i = np.where(y_test == y_i)\n",
    "\n",
    "        # Computes the accuracy\n",
    "        print('class', y_i, 'Accuracy: ', str(round(np.sum(y_test[indices_i] == y_pred[indices_i])/ len(np.where(y_test==y_i)[0]), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b800b521",
   "metadata": {},
   "source": [
    "# Random Forest without SMOTE oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d4e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a27c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 2, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in range(1,20)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(class_weight=\"balanced_subsample\")\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 200, \n",
    "                               cv = 3, \n",
    "                               verbose=3, \n",
    "                               random_state=0, \n",
    "                               n_jobs = -1,\n",
    "                               scoring='balanced_accuracy', \n",
    "                               return_train_score=True)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameter and the best score\n",
    "print(\"\\nBest parameter:\", rf_random.best_params_)\n",
    "print(\"Training set cross-validation balanced accuracy score:\", rf_random.best_score_) \n",
    "print(\"Test set balanced accuracy score:\", balanced_accuracy_score(y_test, rf_random.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e9b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad64c08",
   "metadata": {},
   "source": [
    "# Best parameters\n",
    "\n",
    "* n_estimators: 600,\n",
    "* min_samples_split: 2,\n",
    "* min_samples_leaf: 1,\n",
    "* max_features: 'sqrt',\n",
    "* max_depth: None,\n",
    "* bootstrap: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1cfd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_clf = RandomForestClassifier(n_estimators= 668,\n",
    "                                            min_samples_split= 2,\n",
    "                                            min_samples_leaf= 1,\n",
    "                                            max_features= 'sqrt',\n",
    "                                            max_depth= None,\n",
    "                                            bootstrap= False,\n",
    "                                          class_weight=\"balanced_subsample\")\n",
    "random_forest_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy\")\n",
    "print(\"Training set accuracy score:\", accuracy_score(y_train, \n",
    "                                                random_forest_clf.predict(X_train))) \n",
    "print(\"Test set accuracy score:\", accuracy_score(y_test, \n",
    "                                                random_forest_clf.predict(X_test)))\n",
    "\n",
    "print( )\n",
    "print(\"Balanced Accuracy\")\n",
    "print(\"Training set balanced accuracy score:\", balanced_accuracy_score(y_train, \n",
    "                                                                   random_forest_clf.predict(X_train))) \n",
    "print(\"Test set balanced accuracy score:\", balanced_accuracy_score(y_test, \n",
    "                                                                   random_forest_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3231986",
   "metadata": {},
   "source": [
    "##### Accuracy per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8add1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_per_class(random_forest_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45b7443",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8d3d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the importances of each column \n",
    "importances = random_forest_clf.feature_importances_\n",
    "\n",
    "# Arranges the indices such that the importances are arranged in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Find the corresponding columns\n",
    "cols_feature_importance = df.drop(['state', 'area_code', 'churn'], axis=1).columns[indices].to_numpy()\n",
    "print(cols_feature_importance)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.barh(cols_feature_importance[::-1], importances[indices][::-1])\n",
    "plt.title('Feature Importance, Random Forest Model')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f16ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3a0a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be1f8f66",
   "metadata": {},
   "source": [
    "# Random forest classifier with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f19c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 2, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in range(1,20)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(class_weight=\"balanced_subsample\")\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random_smote = RandomizedSearchCV(estimator = rf, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 200, \n",
    "                               cv = 3, \n",
    "                               verbose=3, \n",
    "                               random_state=0, \n",
    "                               n_jobs = -1,\n",
    "                               scoring='balanced_accuracy', \n",
    "                               return_train_score=True)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Print the best parameter and the best score\n",
    "print(\"\\nBest parameter:\", rf_random_smote.best_params_)\n",
    "print(\"Training set cross-validation balanced accuracy score:\", rf_random_smote.best_score_) \n",
    "print(\"Test set balanced accuracy score:\", balanced_accuracy_score(y_test, rf_random_smote.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

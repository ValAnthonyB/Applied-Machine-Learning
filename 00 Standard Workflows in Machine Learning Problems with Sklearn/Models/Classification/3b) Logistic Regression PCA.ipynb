{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96dbc127",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cb55a2",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7646ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({0: 8116, 2: 8116, 1: 8116})\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "data = pd.read_csv('C:/Users/vabalagon/Desktop/Meta/New Workflow/data/2 data for modeling (With PCA).csv')\n",
    "\n",
    "# Get the features and target variable from the dataframe\n",
    "X = data.drop(['Survey ID', 'Response Date', 'Likelihood to Recommend'], axis=1).to_numpy()\n",
    "y = data['Likelihood to Recommend'].to_numpy()\n",
    "\n",
    "# Split the data into test and train sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.copy(), y, test_size = 0.25, shuffle=True, random_state=42) #, stratify=y_smote\n",
    "\n",
    "# Apply SMOTE oversampling to the TRAINING SET ONLY\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\n",
    "print('Resampled dataset shape %s' % Counter(y_train_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3566c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1711c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "989adcde",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a05ea788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2882e086",
   "metadata": {},
   "source": [
    "##### Plain logistic regression model with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_plain_clf = LogisticRegression(penalty=\"l2\", \n",
    "                                      max_iter=5000, \n",
    "                                      class_weight='balanced')\n",
    "\n",
    "# Note that C is the inverse of the regularization parameter. The smaller C is, the stronger the regularization effect\n",
    "C_range = np.logspace(-3, 3, 7) #np.abs(np.random.normal(1, 3, 20))   #np.logspace(-3, 3, 7)  np.abs(np.random.normal(.3, .5, 20))\n",
    "param_grid = dict(C=C_range) \n",
    "\n",
    "# Grid search step\n",
    "logreg_plain = GridSearchCV(logreg_plain_clf, param_grid, cv=5, scoring='balanced_accuracy', error_score=\"raise\", return_train_score=True, verbose=5)\n",
    "logreg_plain.fit(X_train_smote, y_train_smote) #roc_auc_score, balanced_accuracy\n",
    "\n",
    "# Print the best parameter and the best score\n",
    "print(\"\\nBest parameter:\", logreg_plain.best_params_)\n",
    "print(\"Training set cross-validation balanced accuracy score:\", logreg_plain.best_score_) \n",
    "print(\"Test set balanced accuracy score:\", balanced_accuracy_score(y_test, logreg_plain.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6578a6",
   "metadata": {},
   "source": [
    "##### Accuracy per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg_plain.predict(X_test)\n",
    "\n",
    "for y_i in np.unique(y_test)[::-1]:\n",
    "    print('class: ', y_i)\n",
    "    \n",
    "    # Find the indices of y_i in the true labels\n",
    "    indices_i = np.where(y_test == y_i)\n",
    "    \n",
    "    # Computes the accuracy\n",
    "    print('Accuracy: ', str(round(np.sum(y_test[indices_i] == y_pred[indices_i])/ len(np.where(y_test==y_i)[0]), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d1c907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af774efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66714ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07a8aaba",
   "metadata": {},
   "source": [
    "##### Logistic Regression with Polynomial features and L2 regularization\n",
    "\n",
    "Steps:\n",
    "1. Fit the data X to Polynomial Features\n",
    "2. Split training and testing examples\n",
    "3. Apply SMOTE oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59c1fd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# X_poly_logreg_smote = StandardScaler().fit_transform(X_poly_logreg_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bde77f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({1: 8108, 2: 8108, 0: 8108})\n"
     ]
    }
   ],
   "source": [
    "# Go back to X first and apply Polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_log_reg = PolynomialFeatures(2)\n",
    "X_poly_logreg = poly_log_reg.fit_transform(X)\n",
    "\n",
    "# Apply standard scaling on the dataset\n",
    "X_poly_logreg = StandardScaler().fit_transform(X_poly_logreg)\n",
    "\n",
    "# Test-train split\n",
    "X_train_poly_logreg, X_test_poly_logreg, y_train_poly_logreg, y_test_poly_logreg = train_test_split(X_poly_logreg, \n",
    "                                                                                                    y, \n",
    "                                                                                                    test_size = 0.25)\n",
    "\n",
    "# Apply SMOTE oversampling algorithm\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_train_poly_logreg, y_train_poly_logreg)\n",
    "print('Resampled dataset shape %s' % Counter(y_train_smote))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe68bb",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d2d8bae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END .......C=0.001;, score=(train=0.795, test=0.790) total time=   0.0s\n",
      "[CV 2/5] END .......C=0.001;, score=(train=0.793, test=0.792) total time=   0.0s\n",
      "[CV 3/5] END .......C=0.001;, score=(train=0.793, test=0.792) total time=   0.0s\n",
      "[CV 4/5] END .......C=0.001;, score=(train=0.792, test=0.797) total time=   0.0s\n",
      "[CV 5/5] END .......C=0.001;, score=(train=0.794, test=0.790) total time=   0.0s\n",
      "[CV 1/5] END C=0.00206913808111479;, score=(train=0.798, test=0.792) total time=   0.1s\n",
      "[CV 2/5] END C=0.00206913808111479;, score=(train=0.796, test=0.796) total time=   0.1s\n",
      "[CV 3/5] END C=0.00206913808111479;, score=(train=0.795, test=0.796) total time=   0.1s\n",
      "[CV 4/5] END C=0.00206913808111479;, score=(train=0.795, test=0.800) total time=   0.1s\n",
      "[CV 5/5] END C=0.00206913808111479;, score=(train=0.796, test=0.793) total time=   0.1s\n",
      "[CV 1/5] END C=0.004281332398719396;, score=(train=0.799, test=0.793) total time=   0.1s\n",
      "[CV 2/5] END C=0.004281332398719396;, score=(train=0.797, test=0.798) total time=   0.1s\n",
      "[CV 3/5] END C=0.004281332398719396;, score=(train=0.797, test=0.798) total time=   0.1s\n",
      "[CV 4/5] END C=0.004281332398719396;, score=(train=0.795, test=0.802) total time=   0.1s\n",
      "[CV 5/5] END C=0.004281332398719396;, score=(train=0.797, test=0.795) total time=   0.1s\n",
      "[CV 1/5] END C=0.008858667904100823;, score=(train=0.800, test=0.792) total time=   0.2s\n",
      "[CV 2/5] END C=0.008858667904100823;, score=(train=0.797, test=0.798) total time=   0.2s\n",
      "[CV 3/5] END C=0.008858667904100823;, score=(train=0.797, test=0.797) total time=   0.1s\n",
      "[CV 4/5] END C=0.008858667904100823;, score=(train=0.797, test=0.802) total time=   0.1s\n",
      "[CV 5/5] END C=0.008858667904100823;, score=(train=0.799, test=0.796) total time=   0.2s\n",
      "[CV 1/5] END C=0.018329807108324356;, score=(train=0.801, test=0.791) total time=   0.3s\n",
      "[CV 2/5] END C=0.018329807108324356;, score=(train=0.798, test=0.800) total time=   0.2s\n",
      "[CV 3/5] END C=0.018329807108324356;, score=(train=0.798, test=0.798) total time=   0.2s\n",
      "[CV 4/5] END C=0.018329807108324356;, score=(train=0.797, test=0.803) total time=   0.2s\n",
      "[CV 5/5] END C=0.018329807108324356;, score=(train=0.799, test=0.796) total time=   0.3s\n",
      "[CV 1/5] END C=0.0379269019073225;, score=(train=0.804, test=0.792) total time=   0.4s\n",
      "[CV 2/5] END C=0.0379269019073225;, score=(train=0.800, test=0.801) total time=   0.3s\n",
      "[CV 3/5] END C=0.0379269019073225;, score=(train=0.800, test=0.800) total time=   0.3s\n",
      "[CV 4/5] END C=0.0379269019073225;, score=(train=0.800, test=0.806) total time=   0.3s\n",
      "[CV 5/5] END C=0.0379269019073225;, score=(train=0.801, test=0.800) total time=   0.4s\n",
      "[CV 1/5] END C=0.07847599703514611;, score=(train=0.804, test=0.792) total time=   0.5s\n",
      "[CV 2/5] END C=0.07847599703514611;, score=(train=0.802, test=0.802) total time=   0.5s\n",
      "[CV 3/5] END C=0.07847599703514611;, score=(train=0.802, test=0.800) total time=   0.5s\n",
      "[CV 4/5] END C=0.07847599703514611;, score=(train=0.801, test=0.807) total time=   0.5s\n",
      "[CV 5/5] END C=0.07847599703514611;, score=(train=0.802, test=0.801) total time=   0.5s\n",
      "[CV 1/5] END C=0.1623776739188721;, score=(train=0.804, test=0.793) total time=   0.7s\n",
      "[CV 2/5] END C=0.1623776739188721;, score=(train=0.802, test=0.801) total time=   0.7s\n",
      "[CV 3/5] END C=0.1623776739188721;, score=(train=0.802, test=0.802) total time=   0.6s\n",
      "[CV 4/5] END C=0.1623776739188721;, score=(train=0.800, test=0.807) total time=   0.7s\n",
      "[CV 5/5] END C=0.1623776739188721;, score=(train=0.801, test=0.800) total time=   0.7s\n",
      "[CV 1/5] END C=0.3359818286283781;, score=(train=0.804, test=0.792) total time=   0.9s\n",
      "[CV 2/5] END C=0.3359818286283781;, score=(train=0.803, test=0.803) total time=   0.8s\n",
      "[CV 3/5] END C=0.3359818286283781;, score=(train=0.802, test=0.802) total time=   0.8s\n",
      "[CV 4/5] END C=0.3359818286283781;, score=(train=0.801, test=0.806) total time=   0.9s\n",
      "[CV 5/5] END C=0.3359818286283781;, score=(train=0.801, test=0.801) total time=   0.9s\n",
      "[CV 1/5] END C=0.6951927961775606;, score=(train=0.804, test=0.793) total time=   1.0s\n",
      "[CV 2/5] END C=0.6951927961775606;, score=(train=0.803, test=0.802) total time=   1.0s\n",
      "[CV 3/5] END C=0.6951927961775606;, score=(train=0.802, test=0.802) total time=   1.0s\n",
      "[CV 4/5] END C=0.6951927961775606;, score=(train=0.801, test=0.807) total time=   1.2s\n",
      "[CV 5/5] END C=0.6951927961775606;, score=(train=0.801, test=0.800) total time=   1.1s\n",
      "[CV 1/5] END C=1.438449888287663;, score=(train=0.804, test=0.793) total time=   1.4s\n",
      "[CV 2/5] END C=1.438449888287663;, score=(train=0.803, test=0.801) total time=   1.2s\n",
      "[CV 3/5] END C=1.438449888287663;, score=(train=0.802, test=0.802) total time=   1.3s\n",
      "[CV 4/5] END C=1.438449888287663;, score=(train=0.801, test=0.806) total time=   1.3s\n",
      "[CV 5/5] END C=1.438449888287663;, score=(train=0.801, test=0.800) total time=   1.2s\n",
      "[CV 1/5] END C=2.976351441631316;, score=(train=0.805, test=0.793) total time=   1.4s\n",
      "[CV 2/5] END C=2.976351441631316;, score=(train=0.803, test=0.801) total time=   1.2s\n",
      "[CV 3/5] END C=2.976351441631316;, score=(train=0.802, test=0.802) total time=   1.3s\n",
      "[CV 4/5] END C=2.976351441631316;, score=(train=0.801, test=0.806) total time=   1.4s\n",
      "[CV 5/5] END C=2.976351441631316;, score=(train=0.801, test=0.800) total time=   1.3s\n",
      "[CV 1/5] END C=6.158482110660261;, score=(train=0.805, test=0.793) total time=   1.4s\n",
      "[CV 2/5] END C=6.158482110660261;, score=(train=0.803, test=0.801) total time=   1.2s\n",
      "[CV 3/5] END C=6.158482110660261;, score=(train=0.802, test=0.801) total time=   1.4s\n",
      "[CV 4/5] END C=6.158482110660261;, score=(train=0.801, test=0.806) total time=   1.5s\n",
      "[CV 5/5] END C=6.158482110660261;, score=(train=0.801, test=0.799) total time=   1.4s\n",
      "[CV 1/5] END C=12.742749857031322;, score=(train=0.805, test=0.793) total time=   1.5s\n",
      "[CV 2/5] END C=12.742749857031322;, score=(train=0.802, test=0.802) total time=   1.4s\n",
      "[CV 3/5] END C=12.742749857031322;, score=(train=0.802, test=0.801) total time=   1.4s\n",
      "[CV 4/5] END C=12.742749857031322;, score=(train=0.801, test=0.806) total time=   1.4s\n",
      "[CV 5/5] END C=12.742749857031322;, score=(train=0.801, test=0.799) total time=   1.4s\n",
      "[CV 1/5] END C=26.366508987303554;, score=(train=0.805, test=0.793) total time=   1.5s\n",
      "[CV 2/5] END C=26.366508987303554;, score=(train=0.802, test=0.802) total time=   1.4s\n",
      "[CV 3/5] END C=26.366508987303554;, score=(train=0.802, test=0.801) total time=   1.4s\n",
      "[CV 4/5] END C=26.366508987303554;, score=(train=0.802, test=0.806) total time=   1.5s\n",
      "[CV 5/5] END C=26.366508987303554;, score=(train=0.801, test=0.799) total time=   1.4s\n",
      "[CV 1/5] END C=54.555947811685144;, score=(train=0.805, test=0.793) total time=   1.4s\n",
      "[CV 2/5] END C=54.555947811685144;, score=(train=0.802, test=0.802) total time=   1.4s\n",
      "[CV 3/5] END C=54.555947811685144;, score=(train=0.802, test=0.801) total time=   1.3s\n",
      "[CV 4/5] END C=54.555947811685144;, score=(train=0.802, test=0.805) total time=   1.5s\n",
      "[CV 5/5] END C=54.555947811685144;, score=(train=0.801, test=0.799) total time=   1.4s\n",
      "[CV 1/5] END C=112.88378916846884;, score=(train=0.805, test=0.793) total time=   1.4s\n",
      "[CV 2/5] END C=112.88378916846884;, score=(train=0.802, test=0.802) total time=   1.4s\n",
      "[CV 3/5] END C=112.88378916846884;, score=(train=0.802, test=0.801) total time=   1.4s\n",
      "[CV 4/5] END C=112.88378916846884;, score=(train=0.802, test=0.806) total time=   1.5s\n",
      "[CV 5/5] END C=112.88378916846884;, score=(train=0.801, test=0.799) total time=   1.5s\n",
      "[CV 1/5] END C=233.57214690901213;, score=(train=0.805, test=0.793) total time=   1.4s\n",
      "[CV 2/5] END C=233.57214690901213;, score=(train=0.802, test=0.802) total time=   1.4s\n",
      "[CV 3/5] END C=233.57214690901213;, score=(train=0.802, test=0.801) total time=   1.3s\n",
      "[CV 4/5] END C=233.57214690901213;, score=(train=0.802, test=0.806) total time=   1.5s\n",
      "[CV 5/5] END C=233.57214690901213;, score=(train=0.801, test=0.799) total time=   1.6s\n",
      "[CV 1/5] END C=483.2930238571752;, score=(train=0.805, test=0.793) total time=   1.5s\n",
      "[CV 2/5] END C=483.2930238571752;, score=(train=0.802, test=0.802) total time=   1.4s\n",
      "[CV 3/5] END C=483.2930238571752;, score=(train=0.802, test=0.801) total time=   1.4s\n",
      "[CV 4/5] END C=483.2930238571752;, score=(train=0.802, test=0.805) total time=   1.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END C=483.2930238571752;, score=(train=0.801, test=0.799) total time=   1.5s\n",
      "[CV 1/5] END ......C=1000.0;, score=(train=0.805, test=0.793) total time=   1.5s\n",
      "[CV 2/5] END ......C=1000.0;, score=(train=0.802, test=0.802) total time=   1.4s\n",
      "[CV 3/5] END ......C=1000.0;, score=(train=0.802, test=0.801) total time=   1.4s\n",
      "[CV 4/5] END ......C=1000.0;, score=(train=0.802, test=0.806) total time=   1.6s\n",
      "[CV 5/5] END ......C=1000.0;, score=(train=0.801, test=0.799) total time=   1.4s\n",
      "\n",
      "Best parameter: {'C': 0.6951927961775606}\n"
     ]
    }
   ],
   "source": [
    "logreg_poly_clf = LogisticRegression(penalty=\"l2\", max_iter=3000) #, class_weight='balanced'\n",
    "\n",
    "# Dictionary of possible parameter values per parameter\n",
    "C_range = np.logspace(-3, 3, 20) #np.abs(np.random.normal(0.1, .01, 20))\n",
    "#[0.0004, 0.0005, 0.0006] # np.random.normal(11, .3, 5).tolist() np.logspace(-2, 1, 4).tolist(), [0.02, 0.03, 0.04, 0.05, 0.06]\n",
    "\n",
    "param_grid = dict(C=C_range) # Note that C is the inverse of the regularization parameter\n",
    "\n",
    "# Grid search step\n",
    "logreg_poly = GridSearchCV(logreg_poly_clf, param_grid, cv=5, \n",
    "                           scoring='balanced_accuracy', \n",
    "                           return_train_score=True, \n",
    "                           verbose=5)\n",
    "logreg_poly.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Print the best parameter and the best score\n",
    "print(\"\\nBest parameter:\", logreg_poly.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "681932c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class:  2\n",
      "Accuracy:  0.879\n",
      "class:  1\n",
      "Accuracy:  0.663\n",
      "class:  0\n",
      "Accuracy:  0.874\n"
     ]
    }
   ],
   "source": [
    "y_pred = logreg_poly.predict(X_test_poly_logreg)\n",
    "\n",
    "for y_i in np.unique(y_test_poly_logreg)[::-1]:\n",
    "    print('class: ', y_i)\n",
    "    \n",
    "    # Find the indices of y_i in the true labels\n",
    "    indices_i = np.where(y_test_poly_logreg == y_i)\n",
    "    \n",
    "    # Computes the accuracy\n",
    "    print('Accuracy: ', str(round(np.sum(y_test_poly_logreg[indices_i] == y_pred[indices_i])/ len(np.where(y_test_poly_logreg==y_i)[0]), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec15ea",
   "metadata": {},
   "source": [
    "##### Balanced accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6821cdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set balanced accuracy score: 0.8055270745965964\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set balanced accuracy score:\", balanced_accuracy_score(y_test_poly_logreg, logreg_poly.predict(X_test_poly_logreg)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
